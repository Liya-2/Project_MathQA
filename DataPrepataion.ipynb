{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset statistic ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program Files (x86)\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "D:\\Program Files (x86)\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training problems number:  29837\n",
      "category\n",
      "gain            5120\n",
      "general        13273\n",
      "geometry        2117\n",
      "other           1814\n",
      "physics         7063\n",
      "probability      450\n",
      "Name: Problem, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Rationale</th>\n",
       "      <th>options</th>\n",
       "      <th>correct</th>\n",
       "      <th>annotated_formula</th>\n",
       "      <th>linear_formula</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the banker ' s gain of a certain sum due 3 yea...</td>\n",
       "      <td>\"explanation : t = 3 years r = 10 % td = ( bg ...</td>\n",
       "      <td>a ) rs . 400 , b ) rs . 300 , c ) rs . 500 , d...</td>\n",
       "      <td>a</td>\n",
       "      <td>divide(multiply(const_100, divide(multiply(36,...</td>\n",
       "      <td>multiply(n2,const_100)|multiply(n0,n1)|divide(...</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>average age of students of an adult school is ...</td>\n",
       "      <td>\"explanation : let the original no . of studen...</td>\n",
       "      <td>a ) 1200 , b ) 120 , c ) 360 , d ) 240 , e ) n...</td>\n",
       "      <td>d</td>\n",
       "      <td>multiply(divide(subtract(multiply(add(32, 4), ...</td>\n",
       "      <td>add(n2,n3)|multiply(n1,n2)|multiply(n1,#0)|sub...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sophia finished 2 / 3 of a book . she calculat...</td>\n",
       "      <td>let xx be the total number of pages in the boo...</td>\n",
       "      <td>a ) 229 , b ) 270 , c ) 877 , d ) 266 , e ) 281</td>\n",
       "      <td>b</td>\n",
       "      <td>divide(90, subtract(const_1, divide(2, 3)))</td>\n",
       "      <td>divide(n0,n1)|subtract(const_1,#0)|divide(n2,#1)</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120 is what percent of 50 ?</td>\n",
       "      <td>\"50 * x = 120 - - &gt; x = 2.4 - - &gt; 2.4 expresse...</td>\n",
       "      <td>a ) 5 % , b ) 240 % , c ) 50 % , d ) 2 % , e )...</td>\n",
       "      <td>b</td>\n",
       "      <td>multiply(divide(120, 50), const_100)</td>\n",
       "      <td>divide(n0,n1)|multiply(#0,const_100)|</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there are 10 girls and 20 boys in a classroom ...</td>\n",
       "      <td>if girls is 10 and boys is 20 , then 10 / 20 ....</td>\n",
       "      <td>a ) 1 / 2 , b ) 1 / 3 , c ) 1 / 5 , d ) 10 / 3...</td>\n",
       "      <td>a</td>\n",
       "      <td>divide(10, 20)</td>\n",
       "      <td>divide(n0,n1)</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Problem  \\\n",
       "0  the banker ' s gain of a certain sum due 3 yea...   \n",
       "1  average age of students of an adult school is ...   \n",
       "2  sophia finished 2 / 3 of a book . she calculat...   \n",
       "3                        120 is what percent of 50 ?   \n",
       "4  there are 10 girls and 20 boys in a classroom ...   \n",
       "\n",
       "                                           Rationale  \\\n",
       "0  \"explanation : t = 3 years r = 10 % td = ( bg ...   \n",
       "1  \"explanation : let the original no . of studen...   \n",
       "2  let xx be the total number of pages in the boo...   \n",
       "3  \"50 * x = 120 - - > x = 2.4 - - > 2.4 expresse...   \n",
       "4  if girls is 10 and boys is 20 , then 10 / 20 ....   \n",
       "\n",
       "                                             options correct  \\\n",
       "0  a ) rs . 400 , b ) rs . 300 , c ) rs . 500 , d...       a   \n",
       "1  a ) 1200 , b ) 120 , c ) 360 , d ) 240 , e ) n...       d   \n",
       "2    a ) 229 , b ) 270 , c ) 877 , d ) 266 , e ) 281       b   \n",
       "3  a ) 5 % , b ) 240 % , c ) 50 % , d ) 2 % , e )...       b   \n",
       "4  a ) 1 / 2 , b ) 1 / 3 , c ) 1 / 5 , d ) 10 / 3...       a   \n",
       "\n",
       "                                   annotated_formula  \\\n",
       "0  divide(multiply(const_100, divide(multiply(36,...   \n",
       "1  multiply(divide(subtract(multiply(add(32, 4), ...   \n",
       "2        divide(90, subtract(const_1, divide(2, 3)))   \n",
       "3               multiply(divide(120, 50), const_100)   \n",
       "4                                     divide(10, 20)   \n",
       "\n",
       "                                      linear_formula category  \n",
       "0  multiply(n2,const_100)|multiply(n0,n1)|divide(...     gain  \n",
       "1  add(n2,n3)|multiply(n1,n2)|multiply(n1,#0)|sub...  general  \n",
       "2   divide(n0,n1)|subtract(const_1,#0)|divide(n2,#1)  general  \n",
       "3              divide(n0,n1)|multiply(#0,const_100)|     gain  \n",
       "4                                      divide(n0,n1)    other  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dataset information:\n",
    "        train: 29837\n",
    "        dev: 4475\n",
    "        test: 2985\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# training data\n",
    "\n",
    "with open('dataset/MathQA/train.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "train_df = pd.DataFrame(data)\n",
    "\n",
    "print('training problems number: ', train_df['Problem'].count())\n",
    "print(train_df.groupby(['category'])['Problem'].count())\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev problems number:  4475\n",
      "category\n",
      "gain            743\n",
      "general        2006\n",
      "geometry        308\n",
      "other           288\n",
      "physics        1056\n",
      "probability      74\n",
      "Name: Problem, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Rationale</th>\n",
       "      <th>options</th>\n",
       "      <th>correct</th>\n",
       "      <th>annotated_formula</th>\n",
       "      <th>linear_formula</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a multiple choice test consists of 4 questions...</td>\n",
       "      <td>\"5 choices for each of the 4 questions , thus ...</td>\n",
       "      <td>a ) 24 , b ) 120 , c ) 625 , d ) 720 , e ) 1024</td>\n",
       "      <td>c</td>\n",
       "      <td>power(5, 4)</td>\n",
       "      <td>power(n1,n0)|</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a 3 - digit positive integer is chosen at rand...</td>\n",
       "      <td>p ( three odd digits ) = 5 / 9 * 5 / 10 * 5 / ...</td>\n",
       "      <td>a ) 1 / 2 , b ) 31 / 36 , c ) 49 / 54 , d ) 7 ...</td>\n",
       "      <td>b</td>\n",
       "      <td>subtract(const_1, power(divide(const_1, const_...</td>\n",
       "      <td>divide(const_1,const_2)|power(#0,n0)|subtract(...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if x and y are positive integers and 7 + x + y...</td>\n",
       "      <td>\"try each answer choices . for a : 7 + 3 + xy ...</td>\n",
       "      <td>a ) 3 , b ) 5 , c ) 6 , d ) 8 , e ) 9</td>\n",
       "      <td>c</td>\n",
       "      <td>add(21, const_1)</td>\n",
       "      <td>add(n1,const_1)|</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the hcf and lcm of two numbers m and n are res...</td>\n",
       "      <td>answer we have , m x n = 6 x 210 = 1260 ∴ 1 / ...</td>\n",
       "      <td>a ) 1 / 35 , b ) 3 / 35 , c ) 5 / 37 , d ) 2 /...</td>\n",
       "      <td>d</td>\n",
       "      <td>divide(72, multiply(6, 210))</td>\n",
       "      <td>multiply(n0,n1)|divide(n2,#0)</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in a kilometer race , a beats b by 48 meters o...</td>\n",
       "      <td>\"time taken by b run 1000 meters = ( 1000 * 12...</td>\n",
       "      <td>a ) 238 sec , b ) 190 sec , c ) 667 sec , d ) ...</td>\n",
       "      <td>a</td>\n",
       "      <td>subtract(divide(multiply(const_1, const_1000),...</td>\n",
       "      <td>divide(n0,n1)|multiply(const_1,const_1000)|div...</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Problem  \\\n",
       "0  a multiple choice test consists of 4 questions...   \n",
       "1  a 3 - digit positive integer is chosen at rand...   \n",
       "2  if x and y are positive integers and 7 + x + y...   \n",
       "3  the hcf and lcm of two numbers m and n are res...   \n",
       "4  in a kilometer race , a beats b by 48 meters o...   \n",
       "\n",
       "                                           Rationale  \\\n",
       "0  \"5 choices for each of the 4 questions , thus ...   \n",
       "1  p ( three odd digits ) = 5 / 9 * 5 / 10 * 5 / ...   \n",
       "2  \"try each answer choices . for a : 7 + 3 + xy ...   \n",
       "3  answer we have , m x n = 6 x 210 = 1260 ∴ 1 / ...   \n",
       "4  \"time taken by b run 1000 meters = ( 1000 * 12...   \n",
       "\n",
       "                                             options correct  \\\n",
       "0    a ) 24 , b ) 120 , c ) 625 , d ) 720 , e ) 1024       c   \n",
       "1  a ) 1 / 2 , b ) 31 / 36 , c ) 49 / 54 , d ) 7 ...       b   \n",
       "2              a ) 3 , b ) 5 , c ) 6 , d ) 8 , e ) 9       c   \n",
       "3  a ) 1 / 35 , b ) 3 / 35 , c ) 5 / 37 , d ) 2 /...       d   \n",
       "4  a ) 238 sec , b ) 190 sec , c ) 667 sec , d ) ...       a   \n",
       "\n",
       "                                   annotated_formula  \\\n",
       "0                                        power(5, 4)   \n",
       "1  subtract(const_1, power(divide(const_1, const_...   \n",
       "2                                   add(21, const_1)   \n",
       "3                       divide(72, multiply(6, 210))   \n",
       "4  subtract(divide(multiply(const_1, const_1000),...   \n",
       "\n",
       "                                      linear_formula category  \n",
       "0                                      power(n1,n0)|  general  \n",
       "1  divide(const_1,const_2)|power(#0,n0)|subtract(...  general  \n",
       "2                                   add(n1,const_1)|  general  \n",
       "3                      multiply(n0,n1)|divide(n2,#0)  general  \n",
       "4  divide(n0,n1)|multiply(const_1,const_1000)|div...  physics  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev data\n",
    "\n",
    "with open('dataset/MathQA/dev.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "dev_df = pd.DataFrame(data)\n",
    "\n",
    "print('dev problems number: ', dev_df['Problem'].count())\n",
    "print(dev_df.groupby(['category'])['Problem'].count())\n",
    "\n",
    "dev_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test problems number:  2985\n",
      "category\n",
      "gain            558\n",
      "general        1301\n",
      "geometry        200\n",
      "other           172\n",
      "physics         702\n",
      "probability      52\n",
      "Name: Problem, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Rationale</th>\n",
       "      <th>options</th>\n",
       "      <th>correct</th>\n",
       "      <th>annotated_formula</th>\n",
       "      <th>linear_formula</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a shopkeeper sold an article offering a discou...</td>\n",
       "      <td>\"giving no discount to customer implies sellin...</td>\n",
       "      <td>a ) 38 , b ) 27.675 , c ) 30 , d ) data inadeq...</td>\n",
       "      <td>a</td>\n",
       "      <td>subtract(divide(multiply(add(const_100, 31.1),...</td>\n",
       "      <td>add(n1,const_100)|subtract(const_100,n0)|multi...</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what will be the difference between simple and...</td>\n",
       "      <td>\"s . i . = ( 1000 * 14 * 4 ) / 100 = rs . 560 ...</td>\n",
       "      <td>a ) 129 , b ) 130 , c ) 124 , d ) 133 , e ) 145</td>\n",
       "      <td>a</td>\n",
       "      <td>subtract(subtract(multiply(1000, power(add(div...</td>\n",
       "      <td>divide(n0,const_100)|add(#0,const_1)|multiply(...</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there are 28 stations between hyderabad and ba...</td>\n",
       "      <td>\"the total number of stations = 30 from 30 sta...</td>\n",
       "      <td>a ) 156 , b ) 167 , c ) 870 , d ) 352 , e ) 380</td>\n",
       "      <td>c</td>\n",
       "      <td>multiply(add(28, const_1), add(add(28, const_1...</td>\n",
       "      <td>add(n0,const_1)|add(#0,const_1)|multiply(#0,#1)|</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the present population of a town is 3888 . pop...</td>\n",
       "      <td>\"p = 3888 r = 20 % required population of town...</td>\n",
       "      <td>a ) 2500 , b ) 2100 , c ) 3500 , d ) 3600 , e ...</td>\n",
       "      <td>e</td>\n",
       "      <td>add(3888, divide(multiply(3888, 20), const_100))</td>\n",
       "      <td>multiply(n0,n1)|divide(#0,const_100)|add(n0,#1)|</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the triplicate ratio of 1 : 9 is ?</td>\n",
       "      <td>\"13 : 93 = 1 : 729 answer : e\"</td>\n",
       "      <td>a ) 1 : 0 , b ) 1 : 8 , c ) 1 : 7 , d ) 1 : 2 ...</td>\n",
       "      <td>e</td>\n",
       "      <td>divide(power(const_2.0, 9), power(const_3.0, 9))</td>\n",
       "      <td>power(const_2.0,n1)|power(const_3.0,n1)|divide...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Problem  \\\n",
       "0  a shopkeeper sold an article offering a discou...   \n",
       "1  what will be the difference between simple and...   \n",
       "2  there are 28 stations between hyderabad and ba...   \n",
       "3  the present population of a town is 3888 . pop...   \n",
       "4                 the triplicate ratio of 1 : 9 is ?   \n",
       "\n",
       "                                           Rationale  \\\n",
       "0  \"giving no discount to customer implies sellin...   \n",
       "1  \"s . i . = ( 1000 * 14 * 4 ) / 100 = rs . 560 ...   \n",
       "2  \"the total number of stations = 30 from 30 sta...   \n",
       "3  \"p = 3888 r = 20 % required population of town...   \n",
       "4                     \"13 : 93 = 1 : 729 answer : e\"   \n",
       "\n",
       "                                             options correct  \\\n",
       "0  a ) 38 , b ) 27.675 , c ) 30 , d ) data inadeq...       a   \n",
       "1    a ) 129 , b ) 130 , c ) 124 , d ) 133 , e ) 145       a   \n",
       "2    a ) 156 , b ) 167 , c ) 870 , d ) 352 , e ) 380       c   \n",
       "3  a ) 2500 , b ) 2100 , c ) 3500 , d ) 3600 , e ...       e   \n",
       "4  a ) 1 : 0 , b ) 1 : 8 , c ) 1 : 7 , d ) 1 : 2 ...       e   \n",
       "\n",
       "                                   annotated_formula  \\\n",
       "0  subtract(divide(multiply(add(const_100, 31.1),...   \n",
       "1  subtract(subtract(multiply(1000, power(add(div...   \n",
       "2  multiply(add(28, const_1), add(add(28, const_1...   \n",
       "3   add(3888, divide(multiply(3888, 20), const_100))   \n",
       "4   divide(power(const_2.0, 9), power(const_3.0, 9))   \n",
       "\n",
       "                                      linear_formula category  \n",
       "0  add(n1,const_100)|subtract(const_100,n0)|multi...     gain  \n",
       "1  divide(n0,const_100)|add(#0,const_1)|multiply(...     gain  \n",
       "2   add(n0,const_1)|add(#0,const_1)|multiply(#0,#1)|  physics  \n",
       "3   multiply(n0,n1)|divide(#0,const_100)|add(n0,#1)|     gain  \n",
       "4  power(const_2.0,n1)|power(const_3.0,n1)|divide...    other  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "\n",
    "with open('dataset/MathQA/test.json') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "print('test problems number: ', test_df['Problem'].count())\n",
    "print(test_df.groupby(['category'])['Problem'].count())\n",
    "\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data information:\n",
    "\n",
    "##### count: 29837\n",
    "\n",
    "    gain          5120\n",
    "    general       13273\n",
    "    geometry      2117\n",
    "    other         1814\n",
    "    physics       7063\n",
    "    probability   450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [the, banker, ', s, gain, of, a, certain, sum,...\n",
      "1        [average, age, of, students, of, an, adult, sc...\n",
      "2        [sophia, finished, 2, /, 3, of, a, book, ., sh...\n",
      "3                      [120, is, what, percent, of, 50, ?]\n",
      "4        [there, are, 10, girls, and, 20, boys, in, a, ...\n",
      "                               ...                        \n",
      "29832    [a, man, invests, some, money, partly, in, 10,...\n",
      "29833    [average, of, 10, matches, is, 32, ,, how, man...\n",
      "29834       [solve, below, question, 2, x, +, 1, =, -, 23]\n",
      "29835                     [calculate, 85184, ÷, ?, =, 352]\n",
      "29836    [7.51, 8.22, 7.86, 8.36, 8.09, 7.83, 8.30, 8.0...\n",
      "Name: Problem, Length: 29837, dtype: object\n",
      "0        [multiply(n2,const_100), multiply(n0,n1), divi...\n",
      "1        [add(n2,n3), multiply(n1,n2), multiply(n1,#0),...\n",
      "2        [divide(n0,n1), subtract(const_1,#0), divide(n...\n",
      "3                [divide(n0,n1), multiply(#0,const_100), ]\n",
      "4                                          [divide(n0,n1)]\n",
      "                               ...                        \n",
      "29832    [multiply(n1,const_2), multiply(n3,const_3), d...\n",
      "29833    [add(n1,n2), add(n0,const_1), multiply(n0,n1),...\n",
      "29834              [add(n1,n2), negate(#0), divide(#1,n0)]\n",
      "29835                                  [multiply(n0,n1), ]\n",
      "29836    [multiply(n17,n19), subtract(n16,#0), add(n16,...\n",
      "Name: linear_formula, Length: 29837, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def split_formula(text):\n",
    "    return text.split('|')\n",
    "\n",
    "train_src = train_df['Problem'].apply(tokenize_en)\n",
    "print(train_src)\n",
    "\n",
    "train_trg = train_df['linear_formula'].apply(split_formula)\n",
    "print(train_trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the max length of the problem sentences ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length in tokens:  356\n",
      "Max sentence length:  373\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for i in train_src:\n",
    "    if max_len < len(i):\n",
    "        max_len = len(i) \n",
    "print(\"max length in tokens: \", max_len)\n",
    "\n",
    "\n",
    "for sent in train_df['Problem']:\n",
    "    \n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length in target tokens:  55\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in train_trg:\n",
    "    if max_len < len(i):\n",
    "        max_len = len(i)\n",
    "        \n",
    "print(\"max length in target tokens: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "\n",
    "SRC = data.Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = data.Field(tokenize = split_formula, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'dataset/MathQA/'\n",
    "# train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "#     path=data_path,\n",
    "#     train='train.json',\n",
    "#     validation='dev.json',\n",
    "#     test='test.json',\n",
    "#     format='json',\n",
    "#     fields=[('SRC', SRC), ('TRG', TRG)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_corpus = tokenizer(\n",
    "    text=train_df['Problem'].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation='longest_first',\n",
    "    max_length=400,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_corpus_trg = tokenizer(\n",
    "    text=train_df['Problem'].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation='longest_first',\n",
    "    max_length=400,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_corpus['input_ids']\n",
    "attention_mask = encoded_corpus['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the banker ' s gain of a certain sum due 3 years hence at 10 % per annum is rs . 36 . what is the present worth ?\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print(train_df['Problem'][0])\n",
    "print(len(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(train_df['Problem'][0], padding=\"max_length\", truncation=True)\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input formatting ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.1\n",
    "seed = 2021\n",
    "\n",
    "train_inputs, test_inputs = train_test_split(\n",
    "    input_ids, \n",
    "#     labels, \n",
    "    test_size=test_size, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "train_masks, test_masks = train_test_split(\n",
    "    attention_mask, \n",
    "#     labels, \n",
    "    test_size=test_size, \n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 24181, 11687, 4847, 2373, 128, 120, 1492, 24438, 1104, 170, 1520, 1107, 122, 188, 1204, 1989, 1105, 126, 120, 130, 24438, 1104, 1103, 2735, 1520, 1107, 123, 183, 1181, 1989, 119, 1191, 1175, 1127, 5306, 5097, 8362, 11613, 1170, 123, 183, 1181, 1989, 117, 1293, 1242, 5097, 1127, 1175, 1107, 1103, 1520, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def create_dataloaders(inputs, masks, batch_size):\n",
    "    input_tensor = torch.tensor(inputs)\n",
    "    mask_tensor = torch.tensor(masks)\n",
    "#     labels_tensor = torch.tensor(labels)\n",
    "    dataset = TensorDataset(input_tensor, mask_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = create_dataloaders(train_inputs, train_masks, batch_size)\n",
    "\n",
    "test_dataloader = create_dataloaders(test_inputs, test_masks, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x281aca737b8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the model in PyTorch ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef5dfa65d4947d599857e1816b2db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095d18159e0f4f1080b773d1a8edd93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import CamembertModel, BertModel\n",
    "\n",
    "\n",
    "class CamembertRegressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_rate=0.2, freeze_camembert=False):\n",
    "        super(CamembertRegressor, self).__init__()\n",
    "        D_in, D_out = 768, 1\n",
    "        \n",
    "        self.camembert = CamembertModel.from_pretrained('camembert-base')\n",
    "        self.regressor = nn.Sequential(nn.Dropout(drop_rate),nn.Linear(D_in, D_out))\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        outputs = self.camembert(input_ids, attention_masks)\n",
    "        class_label_output = outputs[1]\n",
    "        outputs = self.regressor(class_label_output)    \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "model = CamembertRegressor(drop_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertRegressor(\n",
       "  (camembert): CamembertModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (regressor): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----\n",
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5560/522798372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5560/522798372.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler, loss_function, epochs, train_dataloader, device, clip_value)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "\n",
    "def train(model, optimizer, scheduler, loss_function, epochs, train_dataloader, device, clip_value=2):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        print(\"-----\")\n",
    "        best_loss = 1e10\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            print(step)  \n",
    "            batch_inputs, batch_masks = tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)           \n",
    "            loss = loss_function(outputs.squeeze())\n",
    "            loss.backward()\n",
    "            clip_grad_norm(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train(model, optimizer, scheduler, loss_function, epochs, train_dataloader, device, clip_value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
